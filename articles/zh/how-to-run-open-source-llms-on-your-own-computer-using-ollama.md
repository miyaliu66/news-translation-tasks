```markdown 
--- 
title: 如何在您自己的计算机上使用 Ollama 运行开源 LLM 
date: 2025-01-07T05:43:05.474Z 
author: Krishna Sarathi Ghosh 
authorURL: https://www.freecodecamp.org/news/author/imkrishnasarathi/ 
originalURL: https://www.freecodecamp.org/news/how-to-run-open-source-llms-on-your-own-computer-using-ollama/ 
posteditor: "" 
proofreader: "" 
--- 
 
如今，AI 工具已经非常普遍，您可能每天都会使用它们。为了保护您的机密数据——包括个人和业务相关的数据——的一个关键方法是，在您自己的基础设施上运行您自己的 AI。 
 
<!-- more --> 
 
本指南将解释如何在您的计算机上托管开源 LLM。这样做可以确保您不会通过基于云的 AI 解决方案将数据泄露给第三方公司。 
 
## 前提条件 
 
-   **一些 AI 知识**：我将涵盖与本文相关的主要概念，但一些关于 LLM 的基本知识将帮助您更好地理解这一点。不过，如果您不了解任何内容也没关系——您可能会发现这仍然很有趣。 
 
-   **一台不错的电脑**：拥有至少 16GB 内存、多核 CPU，最好还有 GPU 的系统，以获得最佳性能。（如果您的规格较低，速度可能会非常慢） 
 
-   **互联网连接**：下载和安装模型需要互联网连接。 
 
-   **时间和耐心** 
 
## 什么是 LLM？ 
 
LLM，即大型语言模型，是经过训练以理解和生成自然人类可读语言的高级 AI 系统。它们使用算法来处理和理解自然语言，并通过大量的信息训练来理解数据中的模式和关系。 
 
像 OpenAI、Anthropic 和 Meta 这样的公司创建了 LLM，您可以使用它们来执行生成内容、分析代码、规划旅行等任务。 
 
## 基于云的 AI 与自托管 AI 
 
在决定本地托管 AI 模型之前，了解这种方法与基于云的解决方案有何不同是很重要的。这两种选择各有优势，适用于不同的使用场景。 
 
### **基于云的 AI 解决方案** 
 
这些服务由 OpenAI、Google 或 AWS 等提供商托管和维护。示例包括 OpenAI 的 GPT 模型、Google Bard 和 AWS SageMaker。您可以通过互联网使用 API 或其端点访问这些模型。 
 
**主要特点**： 
 
-   **易于使用**：设置最小化——您只需集成 API 或通过网页访问。 
     
-   **可扩展性**：由于由公司管理，因此更好地处理大型工作负载和并发请求。 
     
-   **前沿模型**：通常最新和最强大的模型都在云中可用。 
     
-   **数据依赖**：您的数据需要发送到云中进行处理，这可能引发隐私问题。 
     
-   **持续成本**：尽管有些模型是免费的，但其他模型通常按请求或使用计费，尤其是更强大或最新的模型，从而成为运营成本。 
     
 
### **自托管 AI** 
 
使用这种方法，您在自己的硬件上运行模型。可以使用 Ollama 等工具下载和托管开源 LLM，如 Llama 2、GPT-J 或 Mistral。 
 
**主要特点**： 
 
-   **数据隐私**：您的数据保留在您的基础设施上，让您对其有完全的控制权。 
     
-   **从长远来看更具成本效益**：需要在硬件上进行前期投资，但避免了持续的 API 费用。 
     
-   **可定制性**：您可以微调和调整模型以满足特定需求。 
     
-   **技术要求**：需要强大的硬件、设置工作和技术知识。 
     
-   **有限的可扩展性**：最适合个人或小规模使用。 
     
 
### **您应该选择哪种方式？** 
 
如果您需要快速和可扩展地访问高级模型，并且不介意与第三方共享数据，那么基于云的 AI 解决方案可能是更好的选择。另一方面，如果数据安全、定制化或节约成本是首要任务，局部托管 LLM 可能是更好的选择。 
 
## 如何在您的计算机上本地运行 LLM？ 
 
有多种解决方案可以让您在自己的基础设施上运行某些开源 LLM。 
 
虽然大多数本地托管的解决方案专注于**开源 LLM**——如 Llama 2、GPT-J 或 Mistral——但也有部分情况下，根据其使用条款，可以本地运行专有或授权的模型。 
 
-   **开源模型**：这些模型是免费提供的，可以下载、修改和托管，无需许可证限制。示例包括 Llama 2（Meta）、GPT-J 和 Mistral。 
 
-   **具有本地选项的专有模型**：一些公司可能会提供其模型的可下载版本以供离线使用，但这通常需要特定的许可或硬件。例如，NVIDIA 的 NeMo 框架提供工具以在您的基础设施上托管他们的模型，某些小公司可能会为企业客户提供其专有 LLM 的可下载版本。 
 
请记住，如果您运行自己的 LLM，您将需要一台性能强大的计算机（需要良好的 GPU 和 CPU）。如果您的计算机性能不高，您可以尝试运行较小和更轻量级的模型，但速度可能仍然很慢。 
``` 
 
```markdown 
-   CPU: 英特尔 Core i7 13700HX 
 
-   RAM: 16GB DDR5 
 
-   存储: 512GB SSD 
 
-   GPU: 英伟达 RTX 3050 (6GB) 
 
在本指南中，您将使用 Ollama 在您的 PC 上下载和运行 AI 模型。 
 
### 什么是 Ollama？ 
 
[Ollama][1] 是一款工具，旨在简化直接在您的计算机上运行开源大型语言模型（LLM）的过程。它充当本地模型管理器和运行环境，处理从下载模型文件到设置本地交互环境的一切工作。 
 
**Ollama 能帮助您做的事情包括：** 
 
-   **管理您的模型**：Ollama 提供了一种简单的方法来浏览、下载和管理不同的开源模型。您可以在他们的官方网站上查看支持的模型列表。 
     
-   **轻松部署**：只需几个命令，您就可以设置一个功能齐全的环境来运行和交互 LLM。 
     
-   **本地托管**：模型完全在您的基础架构上运行，确保您的数据保持私密和安全。 
     
-   **集成不同的模型**：它支持将模型集成到您自己的项目中，使用诸如 Python 或 JavaScript 等编程语言。 
     
 
通过使用 Ollama，您无需深入了解机器学习框架设置或管理依赖性即可使用 LLM。对于那些希望在没有深厚技术背景的情况下进行 LLM 实验的人来说，它简化了这个过程。 
 
您可以通过他们网站上的**下载**按钮轻松安装 Ollama。 
 
![ollama 官方网站](https://cdn.hashnode.com/res/hashnode/image/upload/v1734604517326/06605e51-4425-4dbe-b8d9-403270eec95b.png) 
 
### 如何使用 Ollama 安装/运行您的模型 
 
安装 Ollama 后，请按照以下步骤安装和使用您的模型： 
 
1.  打开您的浏览器并访问 [localhost:11434][3] 以确保 Ollama 正在运行。 
     
2.  现在，打开命令提示符，输入 `ollama run <model_name>`。在此添加您想要的由 Ollama 支持的模型名称，例如，Llama2（由 Meta 开发）或 Mistral。 
     
    ![命令提示窗口中正在安装 llama2 模型的图片](https://cdn.hashnode.com/res/hashnode/image/upload/v1734604496300/beef69ca-f6e0-44b8-a3a7-ed488e78e776.png) 
     
3.  等待安装过程完成。 
     
4.  在提示符 `>>> Send a message (/? for help)` 中，输入一条消息给 AI，然后按回车。 
     
 
您已成功安装模型，现在可以与它对话了！ 
 
## 使用您新安装的模型构建一个聊天机器人 
 
在您自己的基础设施中运行开源模型，您可以自由地以任何方式更改和使用模型。您甚至可以使用 `ollama` 模块在 Python、JavaScript 以及其他语言中创建本地聊天机器人或个人应用程序。 
 
现在，让我们通过 Python 在几分钟内创建一个聊天机器人。 
 
### 步骤 1：安装 Python 
 
如果您还没有安装 Python，从[官方 Python 网站][4]下载并安装它。为获得最佳兼容性，请避免使用最新的 Python 版本，因为某些模块可能尚未完全支持它。相反，选择最新的稳定版本（通常是在最近版本之前的版本）以确保所有必需模块的顺利运行。 
 
在设置 Python 时，请确保给予安装程序管理员权限并勾选**添加到 PATH**选项。 
 
### 步骤 2：安装 Ollama 
 
现在，您需要在保存文件的目录中打开一个新的终端窗口。您可以在文件资源管理器中打开该目录并**右键单击**，然后单击**在终端中打开**（如果您使用的是 Windows 10 或更早版本，则为**使用命令提示符或 Powershell 打开**）。 
 
输入 `pip install ollama` 并按回车。这将为 Python 安装 `ollama` 模块，这样您就可以从 Python 访问模型和工具提供的功能。等待直到过程完成。 
 
### 步骤 3：添加 Python 代码 
 
继续在文件系统的某处创建一个带有 `.py` 扩展名的 Python 文件，以便您能轻松访问它。使用您最喜欢的代码编辑器打开文件，如果您尚未安装，可以使用浏览器中的 [VS Code][6] 在线版本。 
 
现在，在您的 Python 文件中添加以下代码： 
 
```python 
from ollama import chat 
 
def stream_response(user_input): 
    """在 CLI 中流式显示聊天模型的响应。""" 
    try: 
        print("\nAI: ", end="", flush=True) 
        stream = chat(model='llama2', messages=[{'role': 'user', 'content': user_input}], stream=True) 
        for chunk in stream: 
            content = chunk['message']['content'] 
            print(content, end='', flush=True) 
        print()  
    except Exception as e: 
        print(f"\nError: {str(e)}") 
 
def main(): 
    print("欢迎使用 CLI AI 聊天机器人！输入 'exit' 以退出。\n") 
    while True: 
        user_input = input("You: ") 
        if user_input.lower() in {"exit", "quit"}: 
            print("再见！") 
            break 
        stream_response(user_input) 
 
if __name__ == "__main__": 
    main() 
``` 
``` 
 
```markdown 
-   首先，聊天模块从 `ollama` 库中导入，该库包含预编写的代码以便与您计算机上的 Ollama 应用集成。 
     
-   然后声明一个 `stream_response` 函数，它将您的提示传递给指定的模型，并在生成时逐块流式传输（提供响应块）实时报答给您。 
     
-   接下来在主函数中，欢迎文本被打印到终端。它获取用户输入并将其传递给 `stream_response` 函数，全部包装在 `while True` 或无限循环中。这使我们可以无限制地向 AI 提问，而不导致执行过程中断。我们还注明如果用户输入包含 **exit** 或 **quit**，代码将停止执行。 
     
 
### 第4步：编写提示 
 
现在返回到终端窗口，输入 `python filename.py`，将 `filename` 替换为您设置的实际文件名，然后按 Enter。 
 
您应该会看到提示 `You:`，就像我们在代码中提到的一样。编写您的提示并按 Enter。您应该会看到 AI 响应正在流式传输。要停止执行，请输入提示 `exit`，或关闭终端窗口。 
 
您甚至可以为 JavaScript 或其他任何支持的语言安装模块，并将 AI 集成到您的代码中。请随时查看 [Ollama 官方文档][7] 以了解您可以使用 AI 模型编写什么代码。 
 
## 如何通过微调自定义您的模型 
 
### 什么是微调？ 
 
微调是将预训练语言模型在特定和自定义数据集上进一步训练以实现特定目的的过程。 虽然 LLM 使用庞大的数据集进行训练，但它们可能并不总是完美契合您的需求。 微调可以让您的模型更适合您的特定用例。 
 
### 如何微调模型 
 
微调需要： 
 
-   **一个预训练的模型**：我建议从一个强大的开源 LLM 开始，比如 LLaMA、Mistral 或 Falcon。 
     
-   **质量数据集**：**数据集**是用于训练、测试或评估机器学习模型（包括 LLM）的数据集合。数据集的质量和相关性直接影响模型在特定任务上的表现。请使用与您的领域或任务相关的数据集。例如，如果您希望 AI 撰写博客文章，请对高质量的博客内容进行训练。 
     
-   **足够的资源**：微调需要重新训练模型，这需要大量的计算资源（最好是拥有强大 GPU 的机器）。 
     
 
为了微调你的模型，你可以使用多个工具。[Unsloth][8] 是一个快速选项，允许您用任意数据集微调模型。 
 
## 自托管 LLM 的好处是什么？ 
 
正如我在上面简要讨论的，自己托管 LLM 有多种原因。总结来说，这里有一些最大的好处： 
 
-   增强的数据隐私和安全性，因为您的数据不会离开您的电脑，您可以完全控制它。 
     
-   节省成本，因为您无需定期支付 API 订阅费用。相反，这是一次性投资，帮助您在长期使用中拥有足够强大的基础设施。 
     
-   很好的可定制性，因为您可以通过微调或在自己的数据集上进行训练来定制模型以满足特定需求。 
     
-   较低的延迟 
     
 
## 什么时候您不应该使用自托管 AI？ 
 
但由于多种原因，这可能不适合您。首先，您可能没有运行模型所需的系统资源——可能您不想升级或不能升级。 
 
其次，您可能没有技术知识或时间来设置自己的模型并进行微调。虽然这并不是特别困难，但确实需要一些背景知识和特定技能。如果您不知道如何解决可能出现的问题，这也可能成为一个问题。 
 
您还可能需要模型全天候运行，而您可能没有基础设施来支持它。 
 
这些问题都不是不可克服的，但它们可能为您提供使用基于云的解决方案还是托管自己模型的决策依据。 
 
## 结论 
 
如果您重视数据隐私、成本效益和定制化，自托管 LLM 可能会改变游戏规则。 
 
像 Ollama 这样的工具让将强大的 AI 模型引入自己的基础设施比以往任何时候都更加容易。尽管自托管并非没有挑战，但它让您可以控制数据，并灵活地根据需要调整模型。 
 
只需要确保在决定这样做之前评估自己的技术能力、硬件资源和项目需求。如果您需要可靠性、可扩展性和快速访问最前沿功能，基于云的 LLM 可能仍然是更好的选择。 
 
如果您喜欢本文，不要忘记表示支持，并在 [X][9] 和 [LinkedIn][10] 上关注我以保持联系。此外，我在 [YouTube][11] 上创建了简短但信息丰富的技术内容，所以不要忘记查看我的内容。 
 
感谢您阅读本文！ 
 
[1]: http://ollama.com 
[2]: http://ollama.com 
[3]: http://localhost:11434 
[4]: https://www.python.org/ 
[5]: https://www.python.org/ 
[6]: https://vscode.dev/ 
[7]: https://github.com/ollama/ollama/blob/main/docs/README.md 
[8]: https://unsloth.ai/ 
[9]: https://x.com/Codeskae 
[10]: https://www.linkedin.com/in/imkrishnasarathi/ 
[11]: https://youtube.com/@krishcodes 
``` 
 
 